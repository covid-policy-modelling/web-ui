#!/usr/bin/env node

const fs = require('fs')
const path = require('path')
const https = require('https')
const URL = require('url')
const {DateTime} = require('luxon')
const mysql = require('serverless-mysql')
const d3 = require('d3')
const _ = require('lodash')
const argv = require('yargs')
  .usage('Usage: $0 --cache-dir [dir] --force')

  .help('h')
  .alias('h', 'help')

  .nargs('c', 1)
  .alias('c', 'cache-dir')
  .describe('c', 'Directory to cache results')

  .boolean('d')
  .alias('d', 'dry-run')
  .describe('f', 'Download and process files, but do not store to database')

  .boolean('f')
  .alias('f', 'force')
  .describe('f', 'Force results even though error check has failed')

  .epilog(
    `
  Download public datasets for covid-19 cases and policy interventions, format these datasets, and write them to MySQL.

  Optionally, pass a path to a dir where downloads should be cached and the results should be written as JSON files.

  To store the results in MySQL, set these environment variables:
  DB_HOST, DB_USERNAME, DB_PASSWORD, DB_DATABASE`
  )
  .version(false).argv

require('ts-node').register({
  project: path.join(__dirname, '../tsconfig.json'),
  compilerOptions: {
    module: 'commonjs'
  }
})
const usParser = require('../lib/data/covid-tracking')
const dataGovUKParser = require('../lib/data/data-gov-uk')
const ecdcParser = require('../lib/data/ecdc')
const oxCGRTParser = require('../lib/data/ox-cgrt')
const statePolicyParser = require('../lib/data/state-policy')
const internationalParser = require('../lib/international-interventions-parser')
const validateFetch = require('../lib/validate-fetch')

const cacheDir = argv.cacheDir
if (cacheDir) {
  console.log(`Using cache directory '${cacheDir}'`)
}
const force = argv.force
if (force) {
  console.warn(
    'Forcing results to be saved to database even if there are validation errors'
  )
}
const dryRun = argv.dryRun

const internationalSchoolClosuresURL = `https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c1_school_closing.csv`
const internationalRestrictionsOnGatheringsURL = `https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c4_restrictions_on_gatherings.csv`
const internationalStayAtHomeRequirementsURL = `https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c6_stay_at_home_requirements.csv`

const db = mysql({
  maxRetries: 5,
  config: {
    host: process.env.DB_HOST,
    user: process.env.DB_USERNAME,
    password: process.env.DB_PASSWORD,
    database: process.env.DB_DATABASE,
    ssl: process.env.NODE_ENV === 'production' && {
      ca: fs.readFileSync(
        require.resolve('../lib/BaltimoreCyberTrustRoot.crt.pem'),
        'utf8'
      )
    },
    dateStrings: true
  }
})

async function main() {
  // Fetch the raw data
  const [
    usaCasesJSON,
    ecdcCasesJSON,
    gbCasesJSON,
    gbInterventionsCSV,
    usInterventionsCSV,
    internationalSchoolClosuresCSV,
    internationalRestrictionsOnGatheringsCSV,
    internationalStayAtHomeRequirementsCSV
  ] = await Promise.all([
    fetchCached(usParser.covidTrackingURL, cacheDir),
    fetchCached(ecdcParser.ecdcCasesURL, cacheDir),
    fetchCached(dataGovUKParser.gbDailyCasesURL, cacheDir),
    fetchCached(oxCGRTParser.gbInterventionsURL, cacheDir),
    fetchCached(statePolicyParser.usInterventionsURL, cacheDir),
    fetchCached(internationalSchoolClosuresURL, cacheDir),
    fetchCached(internationalRestrictionsOnGatheringsURL, cacheDir),
    fetchCached(internationalStayAtHomeRequirementsURL, cacheDir)
  ])

  const usCaseRecords = usParser.parse(usaCasesJSON)
  const worldRows = ecdcParser.parse(ecdcCasesJSON)
  const gbCaseRecords = dataGovUKParser.parse(gbCasesJSON)

  const gbInterventionRecords = oxCGRTParser.parse(gbInterventionsCSV)

  const caseRecords = usCaseRecords.concat(gbCaseRecords)

  const usInterventionRecords = statePolicyParser.parse(usInterventionsCSV)

  // parse the international interventions data
  const internationalSchoolClosures = internationalParser.parseCsv(
    internationalSchoolClosuresCSV,
    'SchoolClose',
    2
  )
  const internationalRestrictionsOnGatherings = internationalParser.parseCsv(
    internationalRestrictionsOnGatheringsCSV,
    'GathRestrict10',
    3
  )
  const internationalStayAtHomeRequirements = internationalParser.parseCsv(
    internationalStayAtHomeRequirementsCSV,
    'StayAtHome',
    2
  )

  const allInterventionRecords = usInterventionRecords
    .concat(internationalSchoolClosures)
    .concat(internationalRestrictionsOnGatherings)
    .concat(internationalStayAtHomeRequirements)
    .concat(gbInterventionRecords) // Mario
    .filter(row => !!row.startDate)
    .map(row => [
      row.regionId,
      row.subregionId,
      row.policy,
      row.notes,
      row.source,
      row.issueDate,
      row.startDate,
      row.easeDate,
      row.expirationDate,
      row.endDate
    ])

  if (cacheDir) {
    fs.writeFileSync(
      path.join(cacheDir, 'case-data.json'),
      JSON.stringify(caseRecords, null, 2),
      'utf8'
    )

    fs.writeFileSync(
      path.join(cacheDir, 'intervention-data.json'),
      JSON.stringify(allInterventionRecords, null, 2),
      'utf8'
    )
  }

  if (dryRun) {
    console.warn('Skipping storage of results')
    return
  }

  let isError = false
  try {
    console.log('Inserting case data')

    if (!caseRecords.length) {
      throw new Error('No case data found')
    }

    await db.query('START TRANSACTION')

    // Populate the case_data table
    await db.query('CREATE TABLE case_data_import LIKE case_data')
    await db.query(
      `
        INSERT INTO case_data_import
        (region_id, subregion_id, date, confirmed, recovered, deaths)
        VALUES
        ?
      `,
      [caseRecords]
    )

    await validateFetch.validateTableLength(
      db,
      'case_data',
      'case_data_import',
      force
    )

    console.log(`Saved ${caseRecords.length} records to case_data table...`)
    await db.query(
      'RENAME TABLE case_data TO case_data_old, case_data_import TO case_data'
    )

    await db.query('COMMIT')
  } catch (e) {
    console.error('Failed to insert case data')
    console.error(e)
    isError = true
  } finally {
    await db.query('DROP TABLE IF EXISTS case_data_old')
    await db.query('DROP TABLE IF EXISTS case_data_import')
  }

  try {
    console.log('Inserting interventions data')

    if (!allInterventionRecords.length) {
      throw new Error('No interventions found')
    }

    // INTERVENTIONS DATA
    await db.query('START TRANSACTION')
    // Populate the intervention_data table
    await db.query(
      'CREATE TABLE intervention_data_import LIKE intervention_data'
    )
    await db.query(
      `
        INSERT INTO intervention_data_import
        (
          region_id, subregion_id, policy, notes, source,
          issue_date, start_date, ease_date, expiration_date, end_date
        )
        VALUES
        ?
      `,
      [allInterventionRecords]
    )

    await validateFetch.validateTableLength(
      db,
      'intervention_data',
      'intervention_data_import',
      force
    )

    console.log(
      `Saved ${allInterventionRecords.length} records to interventions table...`
    )
    await db.query(
      'RENAME TABLE intervention_data TO intervention_data_old, intervention_data_import TO intervention_data'
    )

    await db.query('COMMIT')
  } catch (e) {
    console.error('Failed to insert interventions data')
    console.error(e)
    isError = true
  } finally {
    await db.query('DROP TABLE IF EXISTS intervention_data_old')
    await db.query('DROP TABLE IF EXISTS intervention_data_import')
  }

  if (isError) {
    console.error('Failed to complete all tasks')
    process.exit(1)
  }
}

async function fetchCached(url, cacheDir) {
  let cacheFilename = path.basename(url)

  // UK URL not friendly for the file to be saved
  if (url.includes('data.gov.uk')) {
    cacheFilename = 'gb-data.json'
  }

  const cachePath = cacheDir && path.join(cacheDir, cacheFilename)

  if (cachePath && fs.existsSync(cachePath)) {
    console.log(`Using existing download ${cachePath}...`)
    return fs.readFileSync(cachePath, 'utf8')
  } else {
    console.log(`Downloading from ${url}...`)
    let result = ''
    await new Promise((resolve, reject) => {
      fetch(url)
      function fetch(url) {
        https.get(url, res => {
          // Follow path redirects
          if (res.statusCode === 301 || res.statusCode === 302) {
            const oldURL = URL.parse(url)
            const locationURL = URL.parse(res.headers.location)
            oldURL.path = locationURL.path
            oldURL.pathname = locationURL.pathname
            oldURL.href = null
            const redirectURL = URL.format(oldURL)
            console.log(`Redirected\n  from ${url}\n  to ${redirectURL}...`)
            fetch(redirectURL)
          } else {
            res.on('data', chunk => (result += chunk))
            res.on('end', resolve)
            res.on('error', reject)
          }
        })
      }
    })

    if (cacheDir) {
      if (!fs.existsSync(cacheDir)) {
        console.log(`Cache directory ${cacheDir} does not exist. Creating ...`)
        fs.mkdirSync(cacheDir)
      }
    }

    if (cachePath) {
      console.log(`Saving download to ${cachePath}...`)
      fs.writeFileSync(cachePath, result, 'utf8')
    }
    return result
  }
}

main()
  .then(() => {
    process.exit(0)
  })
  .catch(error => {
    console.error(error)
    process.exit(1)
  })
